{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd426cc0-7208-48e0-bbfc-a078bb9af646",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97b18514-d219-4a0a-9e36-939e2225d20a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"imdb.csv\", encoding=\"cp1252\", quoting=0, escapechar=\"\\\\\", on_bad_lines=\"warn\", dtype=str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3cb07ea3-4cd1-4bf5-ab5c-0b3c9163010d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Probably my all-time favorite movie, a story o...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>I sure would like to see a resurrection of a u...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>This show was an amazing, fresh &amp; innovative i...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Encouraged by the positive comments about this...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>If you like original gut wrenching laughter yo...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive\n",
       "5  Probably my all-time favorite movie, a story o...  positive\n",
       "6  I sure would like to see a resurrection of a u...  positive\n",
       "7  This show was an amazing, fresh & innovative i...  negative\n",
       "8  Encouraged by the positive comments about this...  negative\n",
       "9  If you like original gut wrenching laughter yo...  positive"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e515fcb4-c87c-41ec-9456-24c2c9dc3439",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 770 entries, 0 to 769\n",
      "Data columns (total 2 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   review     770 non-null    object\n",
      " 1   sentiment  769 non-null    object\n",
      "dtypes: object(2)\n",
      "memory usage: 12.2+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b684f03e-dd8a-4c3c-ba8a-e2f83af8cfaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(770, 2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec331fb-79a6-417b-aaad-596c84cfdc9c",
   "metadata": {},
   "source": [
    "Lower Casing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0eb1fe70-5a17-48d4-bd08-2bd90de784e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['review']= df['review'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6e98edf2-e7e6-4c01-a9d8-1b6fa11e0753",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      one of the other reviewers has mentioned that ...\n",
       "1      a wonderful little production. <br /><br />the...\n",
       "2      i thought this was a wonderful way to spend ti...\n",
       "3      basically there's a family where a little boy ...\n",
       "4      petter mattei's \"love in the time of money\" is...\n",
       "                             ...                        \n",
       "765    this film can't make up its mind whether its m...\n",
       "766    why, o' why! ...did i pick this one up? well.....\n",
       "767    only a very small child could overlook the abs...\n",
       "768    our teacher showed us this movie in first grad...\n",
       "769                                                a plo\n",
       "Name: review, Length: 770, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['review']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2d97c9b-0bf6-4cf6-ac98-039a354b8f1c",
   "metadata": {},
   "source": [
    "HTML tags removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "003128df-9b76-4369-ad67-05184b80b5fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def remove_html_tags(text):\n",
    "    pattern= re.compile('<.*?>')\n",
    "    return pattern.sub(r'', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "af5937ea-a7c0-48c6-a039-084fea240845",
   "metadata": {},
   "outputs": [],
   "source": [
    "text= \"<html><body><p>Movie 1</p> <p> Actor- Aamir Khan </p><body/><html/>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3d633d9b-4420-4f1f-b546-3e6af684d1af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Movie 1  Actor- Aamir Khan '"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_html_tags(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "79ae618a-fb78-4ad5-9ebe-880f9fcf3383",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      one of the other reviewers has mentioned that ...\n",
       "1      a wonderful little production. the filming tec...\n",
       "2      i thought this was a wonderful way to spend ti...\n",
       "3      basically there's a family where a little boy ...\n",
       "4      petter mattei's \"love in the time of money\" is...\n",
       "                             ...                        \n",
       "765    this film can't make up its mind whether its m...\n",
       "766    why, o' why! ...did i pick this one up? well.....\n",
       "767    only a very small child could overlook the abs...\n",
       "768    our teacher showed us this movie in first grad...\n",
       "769                                                a plo\n",
       "Name: review, Length: 770, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['review'].apply(remove_html_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d74cac18-9100-4368-81d4-91e1deba40b1",
   "metadata": {},
   "source": [
    "Remove URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2c0ed47c-edfe-4666-90b8-c603ae393744",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_url(text):\n",
    "    pattern = re.compile(r'https?://\\S+|www\\.\\S+')  # Corrected regex\n",
    "    return pattern.sub('', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d7141e60-7aa8-44fb-98fc-be166a512093",
   "metadata": {},
   "outputs": [],
   "source": [
    "text1= 'Google now www.google.com'\n",
    "text2= 'https://www.kaggle.com/campusx/notebook8223fc1abb to search check www.google.com'\n",
    "text3 ='http://www.kaggle.com/campusx/notebook8223fc1abb to search check www.google.com '\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "63e11917-b949-4b15-9c1e-39d01524de26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Google now '"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_url(text1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b442e45c-1e46-445d-acca-ae6973799c39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' to search check '"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_url(text2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "36f862ff-0c0e-40ac-9b6f-ea0f2b6917c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' to search check  '"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_url(text3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b76c424-414a-4c17-b12b-3425036f983d",
   "metadata": {},
   "source": [
    "Remove Punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e8a3b649-362d-479d-a882-64606b4512eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string, time\n",
    "string.punctuation\n",
    "exclude= string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d150c327-3774-4fac-a23b-6f88a63b2647",
   "metadata": {},
   "outputs": [],
   "source": [
    "# method build from scratch , not useful for big dataset\n",
    "def remove_punc(text):\n",
    "    for char in exclude:\n",
    "        text= text.replace(char, '')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cae0d2d6-e574-4a63-8a2b-681fd812ff88",
   "metadata": {},
   "outputs": [],
   "source": [
    "text= 'String .With. Punctuation ?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "742c21fa-0e21-465b-b8ba-7678548078e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "String With Punctuation \n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "start= time.time()\n",
    "print(remove_punc(text))\n",
    "time1= time.time()- start\n",
    "print(time1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e5d0c73a-39fc-4b71-a209-b0d9e091f97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# method in one line\n",
    "def remove_punc1(text):\n",
    "    return text.translate(str.maketrans('','', exclude))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d2861c9d-9cac-41ca-92b3-51ebc80f66e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "String With Punctuation \n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "start= time.time()\n",
    "print(remove_punc(text))\n",
    "time2= time.time()- start\n",
    "print(time2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b05c9296-e792-4a63-b01a-7367a7aed3b2",
   "metadata": {},
   "outputs": [
    {
     "ename": "ZeroDivisionError",
     "evalue": "float division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtime1\u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43mtime2\u001b[49m\n",
      "\u001b[1;31mZeroDivisionError\u001b[0m: float division by zero"
     ]
    }
   ],
   "source": [
    "time1/time2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61bddf9c-e9f9-480f-8cb6-b9e4880f6918",
   "metadata": {},
   "source": [
    "Chat Word Treatment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e0b9389f-180e-4878-bdda-083efa02648e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty dictionary\n",
    "slang_dict = {}\n",
    "\n",
    "# Read the file and store key-value pairs\n",
    "with open(\"slang.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    for line in file:\n",
    "        line = line.strip()  # Remove leading/trailing whitespace\n",
    "        if \"=\" in line:  # First check for '=' delimiter\n",
    "            key, value = line.split(\"=\", 1)  # Split only at the first '='\n",
    "        elif \":\" in line:  # If '=' not found, check for ':'\n",
    "            key, value = line.split(\":\", 1)  # Split only at the first ':'\n",
    "        else:\n",
    "            continue  # Skip lines that don’t follow key-value format\n",
    "        \n",
    "        slang_dict[key.strip()] = value.strip()  # Store cleaned key-value pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c8ab92fc-f5c4-4a84-a266-d69532e34dea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'AFAIK': 'As Far As I Know',\n",
       " 'AFK': 'Away From Keyboard',\n",
       " 'ASAP': 'As Soon As Possible',\n",
       " 'ATK': 'At The Keyboard',\n",
       " 'ATM': 'At The Moment',\n",
       " 'A3': 'Anytime, Anywhere, Anyplace',\n",
       " 'BAK': 'Back At Keyboard',\n",
       " 'BBL': 'Be Back Later',\n",
       " 'BBS': 'Be Back Soon',\n",
       " 'BFN': 'Bye For Now',\n",
       " 'B4N': 'Bye For Now',\n",
       " 'BRB': 'Be Right Back',\n",
       " 'BRT': 'Be Right There',\n",
       " 'BTW': 'By The Way',\n",
       " 'B4': 'Before',\n",
       " 'CU': 'See You',\n",
       " 'CUL8R': 'See You Later',\n",
       " 'CYA': 'See You',\n",
       " 'FAQ': 'Frequently Asked Questions',\n",
       " 'FC': 'Fingers Crossed',\n",
       " 'FWIW': \"For What It's Worth\",\n",
       " 'FYI': 'For Your Information',\n",
       " 'GAL': 'Get A Life',\n",
       " 'GG': 'Good Game',\n",
       " 'GN': 'Good Night',\n",
       " 'GMTA': 'Great Minds Think Alike',\n",
       " 'GR8': 'Great!',\n",
       " 'G9': 'Genius',\n",
       " 'IC': 'I See',\n",
       " 'ICQ': 'I Seek you (also a chat program)',\n",
       " 'ILU': 'ILU: I Love You',\n",
       " 'IMHO': 'In My Honest/Humble Opinion',\n",
       " 'IMO': 'In My Opinion',\n",
       " 'IOW': 'In Other Words',\n",
       " 'IRL': 'In Real Life',\n",
       " 'KISS': 'Keep It Simple, Stupid',\n",
       " 'LDR': 'Long Distance Relationship',\n",
       " 'LMAO': 'Laugh My A.. Off',\n",
       " 'LOL': 'Laughing Out Loud',\n",
       " 'LTNS': 'Long Time No See',\n",
       " 'L8R': 'Later',\n",
       " 'MTE': 'My Thoughts Exactly',\n",
       " 'M8': 'Mate',\n",
       " 'NRN': 'No Reply Necessary',\n",
       " 'OIC': 'Oh I See',\n",
       " 'PITA': 'Pain In The A..',\n",
       " 'PRT': 'Party',\n",
       " 'PRW': 'Parents Are Watching',\n",
       " 'ROFL': 'Rolling On The Floor Laughing',\n",
       " 'ROFLOL': 'Rolling On The Floor Laughing Out Loud',\n",
       " 'ROTFLMAO': 'Rolling On The Floor Laughing My A.. Off',\n",
       " 'SK8': 'Skate',\n",
       " 'STATS': 'Your sex and age',\n",
       " 'ASL': 'Age, Sex, Location',\n",
       " 'THX': 'Thank You',\n",
       " 'TTFN': 'Ta-Ta For Now!',\n",
       " 'TTYL': 'Talk To You Later',\n",
       " 'U': 'You',\n",
       " 'U2': 'You Too',\n",
       " 'U4E': 'Yours For Ever',\n",
       " 'WB': 'Welcome Back',\n",
       " 'WTF': 'What The F...',\n",
       " 'WTG': 'Way To Go!',\n",
       " 'WUF': 'Where Are You From?',\n",
       " 'W8': 'Wait...',\n",
       " '7K': 'Sick:-D Laugher',\n",
       " 'BFF': 'Best friends forever'}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "slang_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3d14fbb3-408f-486d-a1f7-323ec48ed6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_conversion(text):\n",
    "    new_text=[]\n",
    "    for w in text.split():\n",
    "        if w.upper() in slang_dict:\n",
    "            new_text.append(slang_dict[w.upper()])\n",
    "        else:\n",
    "            new_text.append(w)\n",
    "    return \" \".join(new_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7d5d0a2e-b107-4f7a-958c-77fc8f91ad42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'In My Honest/Humble Opinion he is the best'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_conversion('IMHO he is the best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c6c5cee7-7695-49d7-8c5f-9f29a8776a38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'For Your Information delhi is the capital of India'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_conversion('FYI delhi is the capital of India')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e3cc28b-43ef-40a8-8724-1081ce30659a",
   "metadata": {},
   "source": [
    "Spelling Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "011ee737-3825-4dfe-b92b-d2b0c8ffa0da",
   "metadata": {},
   "outputs": [],
   "source": [
    "incorrect_text = 'ceertain conditioans during seveal ggenerations are moodified in the saame maner.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ba5e9a84-a867-475d-bfb8-ebd6a528be4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f4a07b83-77cb-4dae-8425-aaed396f7be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "textBlb = TextBlob(incorrect_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f1bf3c43-f725-4c1c-898a-14593bec3db8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextBlob(\"certain conditions during several generations are modified in the same manner.\")"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textBlb.correct()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02da94b2-e543-47dd-9217-75712db8ca5f",
   "metadata": {},
   "source": [
    "Removing Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8cf891dd-fba7-4430-97ab-1165a3e083b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "94ee8ae4-a2ea-4c10-a1eb-acf3f52a312b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text):\n",
    "    new_text=[]\n",
    "    for word in text.split():\n",
    "        if word in stopwords.words('english'):\n",
    "            new_text.append('')\n",
    "        else:\n",
    "            new_text.append(word)\n",
    "    x= new_text[:]\n",
    "    new_text.clear()\n",
    "    return \" \".join(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3eaddf8d-0a82-4798-bc1d-0c0b90f76120",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This   best book   shelf,  I really like it.'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_stopwords('This is the best book on the shelf, and I really like it.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f046aab7-5209-4c13-9fc2-1e5eebd0f23f",
   "metadata": {},
   "source": [
    "Handling Emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5196a01c-0efd-42fd-8335-17f68543b9ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove emojis\n",
    "def remove_emoji(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                               u\"\\U0001F600-\\U0001F64F\"  # Emoticons\n",
    "                               u\"\\U0001F300-\\U0001F5FF\"  # Symbols & pictographs\n",
    "                               u\"\\U0001F680-\\U0001F6FF\"  # Transport & map symbols\n",
    "                               u\"\\U0001F700-\\U0001F77F\"  # Alchemical symbols\n",
    "                               u\"\\U0001F780-\\U0001F7FF\"  # Geometric shapes\n",
    "                               u\"\\U0001F800-\\U0001F8FF\"  # Supplemental arrows\n",
    "                               u\"\\U0001F900-\\U0001F9FF\"  # Supplemental symbols and pictographs\n",
    "                               u\"\\U0001FA00-\\U0001FA6F\"  # Symbols and pictographs extended-A\n",
    "                               u\"\\U0001FA70-\\U0001FAFF\"  # Symbols and pictographs extended-B\n",
    "                               u\"\\U00002702-\\U000027B0\"  # Dingbats\n",
    "                               u\"\\U000024C2-\\U0001F251\"  # Enclosed characters\n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "    \n",
    "    return emoji_pattern.sub(r'', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "85fbf4bb-0971-47be-8d84-eb701b11b9f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello ! This is a test .\n"
     ]
    }
   ],
   "source": [
    "text = \"Hello 😊! This is a test 🚀.\"\n",
    "print(remove_emoji(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "58d521b1-1095-4f79-b75d-5edf9a22e688",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: emoji in c:\\users\\admin\\appdata\\roaming\\python\\python311\\site-packages (2.14.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEPRECATION: Loading egg at c:\\programdata\\anaconda3\\lib\\site-packages\\vboxapi-1.0-py3.11.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation.. Discussion can be found at https://github.com/pypa/pip/issues/12330\n"
     ]
    }
   ],
   "source": [
    "!pip install emoji\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3ab66859-e3ef-48b3-9d0b-9831d8168d33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I love you !:red_heart::kissing_face_with_closed_eyes:\n"
     ]
    }
   ],
   "source": [
    "#replace the emojis with words\n",
    "import emoji\n",
    "print(emoji.demojize('I love you !❤️😚'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33344ed0-aadb-419f-bef1-8dae0999b354",
   "metadata": {},
   "source": [
    "Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558fc6da-3ca1-4122-8f60-697c4bb6e9e1",
   "metadata": {},
   "source": [
    "1. Using Split Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3f3c39f9-f732-4a50-aa7d-7d6c30f750c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'am', 'going', 'to', 'delhi']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent1='I am going to delhi'\n",
    "sent1.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a83d31ce-adf2-4c0c-9358-32c8e48fb015",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I am going to delhi',\n",
       " ' I will stay there for 3 days',\n",
       " \" Let's hope the trip to be great\"]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent2='I am going to delhi. I will stay there for 3 days. Let\\'s hope the trip to be great'\n",
    "sent2.split('.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e9655a9d-5fdd-4d37-aeac-5e319555ae42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'am', 'going', 'to', 'delhi!']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#problems with split function\n",
    "sent3= 'I am going to delhi!'\n",
    "sent3.split()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "16a6751e-cd4f-45ce-b17a-17f5f91069be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Where do think I should go ? I have 3 days holiday']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent4='Where do think I should go ? I have 3 days holiday'\n",
    "sent4.split('.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe320fc2-a672-4de6-817d-3f0d19e874ab",
   "metadata": {},
   "source": [
    "2. Using Regular Expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "48c19e8b-3a00-4364-817e-d0eb13d1102b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'am', 'going', 'to', 'Delhi']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "sent3 = 'I am going to Delhi!'\n",
    "tokens = re.findall(r\"\\w+\", sent3)\n",
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "865dc2f5-017d-4c9f-b9c5-28096b35af78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello', ' How are you', ' I am going to Delhi', \" It's a beautiful city\", '']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"\"\"Hello! How are you? I am going to Delhi. It's a beautiful city.\"\"\"\n",
    "sentences = re.compile('[.!?]').split(text)\n",
    "sentences\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2dcb0bf-cc92-4fc8-85ad-1b8022f5c176",
   "metadata": {},
   "source": [
    "3. Using NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "20e20c29-357a-4781-bb81-4aa262efb9bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f3450e97-6fb9-41e4-95e8-4fda1c4c41d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ADMIN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "01c5d28d-e895-4aa5-8cb9-e32376e45837",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\ADMIN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt_tab')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e4b0c995-f8a2-4c19-929e-f998de8889ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'am', 'going', 'to', 'visit', 'delhi', '!']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent1='I am going to visit delhi!'\n",
    "word_tokenize(sent1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "956186ba-5261-4f88-9a34-6349784b8937",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello!', 'How are you?', 'I am going to Delhi.', \"It's a beautiful city.\"]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"\"\"Hello! How are you? I am going to Delhi. It's a beautiful city.\"\"\"\n",
    "sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "9d523d76-101f-4ca5-88e1-d751e0e53ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent5 = 'I have a Ph.D in A.I'\n",
    "sent6=\"We're here to help ! mail us at nks@gmail.com\"\n",
    "sent7='A 5km ride cost $10.50'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "8a744c2a-eca6-4da6-8385-958c4c0578c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'have', 'a', 'Ph.D', 'in', 'A.I']"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(sent5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "1ec8c222-ea1e-4a4b-ab3e-7fbebb705219",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['We',\n",
       " \"'re\",\n",
       " 'here',\n",
       " 'to',\n",
       " 'help',\n",
       " '!',\n",
       " 'mail',\n",
       " 'us',\n",
       " 'at',\n",
       " 'nks',\n",
       " '@',\n",
       " 'gmail.com']"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(sent6) #problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "b524863a-52d6-4588-8a73-28cab855756b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A', '5km', 'ride', 'cost', '$', '10.50']"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(sent7)  #problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17db4708-b7aa-4df7-a327-46eddb50bb7f",
   "metadata": {},
   "source": [
    "4. Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "813e0594-b4d4-482c-88ba-ddefc98b8ec2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting spacy\n",
      "  Downloading spacy-3.8.4-cp311-cp311-win_amd64.whl.metadata (27 kB)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.11 (from spacy)\n",
      "  Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0 (from spacy)\n",
      "  Downloading spacy_loggers-1.0.5-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0 (from spacy)\n",
      "  Downloading murmurhash-1.0.12-cp311-cp311-win_amd64.whl.metadata (2.2 kB)\n",
      "Collecting cymem<2.1.0,>=2.0.2 (from spacy)\n",
      "  Downloading cymem-2.0.11-cp311-cp311-win_amd64.whl.metadata (8.8 kB)\n",
      "Collecting preshed<3.1.0,>=3.0.2 (from spacy)\n",
      "  Downloading preshed-3.0.9-cp311-cp311-win_amd64.whl.metadata (2.2 kB)\n",
      "Collecting thinc<8.4.0,>=8.3.4 (from spacy)\n",
      "  Downloading thinc-8.3.4-cp311-cp311-win_amd64.whl.metadata (15 kB)\n",
      "Collecting wasabi<1.2.0,>=0.9.1 (from spacy)\n",
      "  Downloading wasabi-1.1.3-py3-none-any.whl.metadata (28 kB)\n",
      "Collecting srsly<3.0.0,>=2.4.3 (from spacy)\n",
      "  Downloading srsly-2.5.1-cp311-cp311-win_amd64.whl.metadata (20 kB)\n",
      "Collecting catalogue<2.1.0,>=2.0.6 (from spacy)\n",
      "  Downloading catalogue-2.0.10-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting weasel<0.5.0,>=0.1.0 (from spacy)\n",
      "  Downloading weasel-0.4.1-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting typer<1.0.0,>=0.3.0 (from spacy)\n",
      "  Downloading typer-0.15.1-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy) (4.65.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy) (1.26.4)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy) (1.10.12)\n",
      "Requirement already satisfied: jinja2 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy) (3.1.3)\n",
      "Requirement already satisfied: setuptools in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy) (68.2.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy) (23.1)\n",
      "Collecting langcodes<4.0.0,>=3.2.0 (from spacy)\n",
      "  Downloading langcodes-3.5.0-py3-none-any.whl.metadata (29 kB)\n",
      "Collecting language-data>=1.2 (from langcodes<4.0.0,>=3.2.0->spacy)\n",
      "  Downloading language_data-1.3.0-py3-none-any.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.2.2)\n",
      "Collecting blis<1.3.0,>=1.2.0 (from thinc<8.4.0,>=8.3.4->spacy)\n",
      "  Downloading blis-1.2.0-cp311-cp311-win_amd64.whl.metadata (7.9 kB)\n",
      "Collecting confection<1.0.0,>=0.0.1 (from thinc<8.4.0,>=8.3.4->spacy)\n",
      "  Downloading confection-0.1.5-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.7)\n",
      "Collecting shellingham>=1.3.0 (from typer<1.0.0,>=0.3.0->spacy)\n",
      "  Downloading shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (13.3.5)\n",
      "Collecting cloudpathlib<1.0.0,>=0.7.0 (from weasel<0.5.0,>=0.1.0->spacy)\n",
      "  Downloading cloudpathlib-0.20.0-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (5.2.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from jinja2->spacy) (2.1.3)\n",
      "Collecting marisa-trie>=1.1.0 (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy)\n",
      "  Downloading marisa_trie-1.2.1-cp311-cp311-win_amd64.whl.metadata (9.3 kB)\n",
      "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\admin\\appdata\\roaming\\python\\python311\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.17.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from markdown-it-py<3.0.0,>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.0)\n",
      "Downloading spacy-3.8.4-cp311-cp311-win_amd64.whl (12.2 MB)\n",
      "   ---------------------------------------- 0.0/12.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/12.2 MB 1.3 MB/s eta 0:00:10\n",
      "   ---------------------------------------- 0.1/12.2 MB 907.3 kB/s eta 0:00:14\n",
      "    --------------------------------------- 0.2/12.2 MB 1.5 MB/s eta 0:00:09\n",
      "   - -------------------------------------- 0.3/12.2 MB 1.8 MB/s eta 0:00:07\n",
      "   - -------------------------------------- 0.5/12.2 MB 2.5 MB/s eta 0:00:05\n",
      "   --- ------------------------------------ 1.0/12.2 MB 3.5 MB/s eta 0:00:04\n",
      "   ---- ----------------------------------- 1.2/12.2 MB 3.9 MB/s eta 0:00:03\n",
      "   ------ --------------------------------- 1.8/12.2 MB 5.1 MB/s eta 0:00:03\n",
      "   -------- ------------------------------- 2.5/12.2 MB 6.1 MB/s eta 0:00:02\n",
      "   --------- ------------------------------ 3.0/12.2 MB 6.7 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 3.7/12.2 MB 7.4 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 4.4/12.2 MB 8.0 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 5.0/12.2 MB 8.5 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 5.7/12.2 MB 8.8 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 6.4/12.2 MB 9.3 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 7.0/12.2 MB 9.6 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 7.1/12.2 MB 9.6 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 7.4/12.2 MB 8.9 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 8.0/12.2 MB 9.2 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 8.7/12.2 MB 9.5 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 9.3/12.2 MB 9.6 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 10.0/12.2 MB 9.8 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 10.6/12.2 MB 11.7 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 11.3/12.2 MB 12.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  12.0/12.2 MB 12.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 12.2/12.2 MB 12.6 MB/s eta 0:00:00\n",
      "Downloading catalogue-2.0.10-py3-none-any.whl (17 kB)\n",
      "Downloading cymem-2.0.11-cp311-cp311-win_amd64.whl (39 kB)\n",
      "Downloading langcodes-3.5.0-py3-none-any.whl (182 kB)\n",
      "   ---------------------------------------- 0.0/183.0 kB ? eta -:--:--\n",
      "   --------------------------------------- 183.0/183.0 kB 10.8 MB/s eta 0:00:00\n",
      "Downloading murmurhash-1.0.12-cp311-cp311-win_amd64.whl (25 kB)\n",
      "Downloading preshed-3.0.9-cp311-cp311-win_amd64.whl (122 kB)\n",
      "   ---------------------------------------- 0.0/122.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 122.3/122.3 kB 7.0 MB/s eta 0:00:00\n",
      "Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
      "Downloading spacy_loggers-1.0.5-py3-none-any.whl (22 kB)\n",
      "Downloading srsly-2.5.1-cp311-cp311-win_amd64.whl (632 kB)\n",
      "   ---------------------------------------- 0.0/632.6 kB ? eta -:--:--\n",
      "   ----------------------------------- --- 573.4/632.6 kB 17.6 MB/s eta 0:00:01\n",
      "   --------------------------------------- 632.6/632.6 kB 13.2 MB/s eta 0:00:00\n",
      "Downloading thinc-8.3.4-cp311-cp311-win_amd64.whl (1.5 MB)\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   --------- ------------------------------ 0.3/1.5 MB 10.6 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 0.7/1.5 MB 10.6 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 0.7/1.5 MB 10.6 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 0.7/1.5 MB 10.6 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 0.7/1.5 MB 10.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.5/1.5 MB 5.9 MB/s eta 0:00:00\n",
      "Downloading typer-0.15.1-py3-none-any.whl (44 kB)\n",
      "   ---------------------------------------- 0.0/44.9 kB ? eta -:--:--\n",
      "   ---------------------------------------- 44.9/44.9 kB 2.2 MB/s eta 0:00:00\n",
      "Downloading wasabi-1.1.3-py3-none-any.whl (27 kB)\n",
      "Downloading weasel-0.4.1-py3-none-any.whl (50 kB)\n",
      "   ---------------------------------------- 0.0/50.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 50.3/50.3 kB 2.5 MB/s eta 0:00:00\n",
      "Downloading blis-1.2.0-cp311-cp311-win_amd64.whl (6.2 MB)\n",
      "   ---------------------------------------- 0.0/6.2 MB ? eta -:--:--\n",
      "   ---- ----------------------------------- 0.7/6.2 MB 15.0 MB/s eta 0:00:01\n",
      "   -------- ------------------------------- 1.3/6.2 MB 13.9 MB/s eta 0:00:01\n",
      "   ------------ --------------------------- 1.9/6.2 MB 13.4 MB/s eta 0:00:01\n",
      "   --------------- ------------------------ 2.4/6.2 MB 12.8 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 3.1/6.2 MB 13.1 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 3.7/6.2 MB 13.2 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 4.4/6.2 MB 13.5 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 5.1/6.2 MB 13.5 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 5.6/6.2 MB 13.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  6.2/6.2 MB 13.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 6.2/6.2 MB 12.9 MB/s eta 0:00:00\n",
      "Downloading cloudpathlib-0.20.0-py3-none-any.whl (52 kB)\n",
      "   ---------------------------------------- 0.0/52.5 kB ? eta -:--:--\n",
      "   ---------------------------------------- 52.5/52.5 kB 2.8 MB/s eta 0:00:00\n",
      "Downloading confection-0.1.5-py3-none-any.whl (35 kB)\n",
      "Downloading language_data-1.3.0-py3-none-any.whl (5.4 MB)\n",
      "   ---------------------------------------- 0.0/5.4 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 0.5/5.4 MB 15.2 MB/s eta 0:00:01\n",
      "   ------- -------------------------------- 1.0/5.4 MB 12.7 MB/s eta 0:00:01\n",
      "   ------------ --------------------------- 1.7/5.4 MB 13.4 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 2.3/5.4 MB 13.3 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 3.0/5.4 MB 13.7 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 3.7/5.4 MB 13.8 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 4.3/5.4 MB 13.8 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 5.0/5.4 MB 13.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 5.4/5.4 MB 13.2 MB/s eta 0:00:00\n",
      "Downloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Downloading marisa_trie-1.2.1-cp311-cp311-win_amd64.whl (152 kB)\n",
      "   ---------------------------------------- 0.0/152.0 kB ? eta -:--:--\n",
      "   ---------------------------------------- 152.0/152.0 kB ? eta 0:00:00\n",
      "Installing collected packages: cymem, wasabi, spacy-loggers, spacy-legacy, shellingham, murmurhash, marisa-trie, cloudpathlib, catalogue, blis, srsly, preshed, language-data, typer, langcodes, confection, weasel, thinc, spacy\n",
      "Successfully installed blis-1.2.0 catalogue-2.0.10 cloudpathlib-0.20.0 confection-0.1.5 cymem-2.0.11 langcodes-3.5.0 language-data-1.3.0 marisa-trie-1.2.1 murmurhash-1.0.12 preshed-3.0.9 shellingham-1.5.4 spacy-3.8.4 spacy-legacy-3.0.12 spacy-loggers-1.0.5 srsly-2.5.1 thinc-8.3.4 typer-0.15.1 wasabi-1.1.3 weasel-0.4.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEPRECATION: Loading egg at c:\\programdata\\anaconda3\\lib\\site-packages\\vboxapi-1.0-py3.11.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation.. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "  WARNING: The script typer.exe is installed in 'C:\\Users\\ADMIN\\AppData\\Roaming\\Python\\Python311\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script weasel.exe is installed in 'C:\\Users\\ADMIN\\AppData\\Roaming\\Python\\Python311\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script spacy.exe is installed in 'C:\\Users\\ADMIN\\AppData\\Roaming\\Python\\Python311\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n"
     ]
    }
   ],
   "source": [
    "pip install spacy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "5c0f432c-1273-42d0-ac2f-744413db4c92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "     --------------------------------------- 0.0/12.8 MB 393.8 kB/s eta 0:00:33\n",
      "     --------------------------------------- 0.1/12.8 MB 573.4 kB/s eta 0:00:23\n",
      "     --------------------------------------- 0.1/12.8 MB 602.4 kB/s eta 0:00:22\n",
      "      -------------------------------------- 0.2/12.8 MB 919.0 kB/s eta 0:00:14\n",
      "      -------------------------------------- 0.3/12.8 MB 947.5 kB/s eta 0:00:14\n",
      "     - -------------------------------------- 0.5/12.8 MB 1.5 MB/s eta 0:00:08\n",
      "     - -------------------------------------- 0.6/12.8 MB 1.4 MB/s eta 0:00:09\n",
      "     -- ------------------------------------- 0.9/12.8 MB 2.1 MB/s eta 0:00:06\n",
      "     --- ------------------------------------ 1.1/12.8 MB 2.4 MB/s eta 0:00:05\n",
      "     ---- ----------------------------------- 1.4/12.8 MB 2.8 MB/s eta 0:00:04\n",
      "     ------ --------------------------------- 1.9/12.8 MB 3.4 MB/s eta 0:00:04\n",
      "     ------- -------------------------------- 2.3/12.8 MB 3.7 MB/s eta 0:00:03\n",
      "     -------- ------------------------------- 2.8/12.8 MB 4.3 MB/s eta 0:00:03\n",
      "     ---------- ----------------------------- 3.2/12.8 MB 4.7 MB/s eta 0:00:03\n",
      "     ----------- ---------------------------- 3.8/12.8 MB 5.1 MB/s eta 0:00:02\n",
      "     ------------ --------------------------- 4.2/12.8 MB 5.3 MB/s eta 0:00:02\n",
      "     -------------- ------------------------- 4.6/12.8 MB 5.5 MB/s eta 0:00:02\n",
      "     ---------------- ----------------------- 5.2/12.8 MB 5.9 MB/s eta 0:00:02\n",
      "     ----------------- ---------------------- 5.7/12.8 MB 6.2 MB/s eta 0:00:02\n",
      "     ------------------- -------------------- 6.3/12.8 MB 6.5 MB/s eta 0:00:01\n",
      "     -------------------- ------------------- 6.6/12.8 MB 6.5 MB/s eta 0:00:01\n",
      "     ---------------------- ----------------- 7.2/12.8 MB 6.8 MB/s eta 0:00:01\n",
      "     ------------------------ --------------- 7.7/12.8 MB 6.9 MB/s eta 0:00:01\n",
      "     ------------------------- -------------- 8.2/12.8 MB 7.2 MB/s eta 0:00:01\n",
      "     --------------------------- ------------ 8.7/12.8 MB 7.2 MB/s eta 0:00:01\n",
      "     ----------------------------- ---------- 9.3/12.8 MB 7.4 MB/s eta 0:00:01\n",
      "     ------------------------------ --------- 9.9/12.8 MB 7.6 MB/s eta 0:00:01\n",
      "     -------------------------------- ------- 10.4/12.8 MB 9.1 MB/s eta 0:00:01\n",
      "     --------------------------------- ----- 11.0/12.8 MB 10.4 MB/s eta 0:00:01\n",
      "     ----------------------------------- --- 11.7/12.8 MB 11.1 MB/s eta 0:00:01\n",
      "     ------------------------------------- - 12.2/12.8 MB 11.1 MB/s eta 0:00:01\n",
      "     --------------------------------------  12.7/12.8 MB 11.5 MB/s eta 0:00:01\n",
      "     --------------------------------------- 12.8/12.8 MB 10.7 MB/s eta 0:00:00\n",
      "Installing collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.8.0\n",
      "\u001b[38;5;2m[+] Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEPRECATION: Loading egg at c:\\programdata\\anaconda3\\lib\\site-packages\\vboxapi-1.0-py3.11.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation.. Discussion can be found at https://github.com/pypa/pip/issues/12330\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "82400240-3ceb-4b5e-8a8a-c3c5efc5ac70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp= spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "e8fe242c-2aab-4e5b-9582-cbc5dd295cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc1=nlp(sent5)\n",
    "doc2=nlp(sent6)\n",
    "doc3=nlp(sent7)\n",
    "doc4=nlp(sent1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "e42c393d-c802-403b-9d1c-a17034011251",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I\n",
      "have\n",
      "a\n",
      "Ph\n",
      ".\n",
      "D\n",
      "in\n",
      "A.I\n"
     ]
    }
   ],
   "source": [
    "for token in doc1:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "28bf6022-842c-4c52-b540-72f36a863260",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We\n",
      "'re\n",
      "here\n",
      "to\n",
      "help\n",
      "!\n",
      "mail\n",
      "us\n",
      "at\n",
      "nks@gmail.com\n"
     ]
    }
   ],
   "source": [
    "for token in doc2:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "47f366a3-8e7b-46e7-8737-96c80e6848ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A\n",
      "5\n",
      "km\n",
      "ride\n",
      "cost\n",
      "$\n",
      "10.50\n"
     ]
    }
   ],
   "source": [
    "for token in doc3:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "6d724544-1baf-4170-9605-a0d896b88196",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I\n",
      "am\n",
      "going\n",
      "to\n",
      "visit\n",
      "delhi\n",
      "!\n"
     ]
    }
   ],
   "source": [
    "for token in doc4:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "a47c2bf4-d551-46dd-b5de-abcc2b4c6f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "ps= PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "8c5d98fa-6ea6-4c6d-a40e-f3f36f859e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem_words(text):\n",
    "    return \" \".join([ps.stem(word) for word in text.split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "d5831756-b066-4376-bbc2-9fa785e6712c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'walk walk walk walk'"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample= \"walk walks walking walked\"\n",
    "stem_words(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "3f93d405-f60e-4489-a676-25a25014f0ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "text=\"\"\"Artificial Intelligence is transforming the world rapidly. Businesses are using AI to automate tasks and improve efficiency. In the future, AI will play an even bigger role in our daily lives. \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "c8ba781c-d169-409f-b024-349c2e4290d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Artificial Intelligence is transforming the world rapidly. Businesses are using AI to automate tasks and improve efficiency. In the future, AI will play an even bigger role in our daily lives. \n"
     ]
    }
   ],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "d8aa1f86-7020-4a3d-aac0-ddeec885a4e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'artifici intellig is transform the world rapidly. busi are use ai to autom task and improv efficiency. in the future, ai will play an even bigger role in our daili lives.'"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stem_words(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92bb326e-6ea8-4b40-9921-2cc4119c6bd0",
   "metadata": {},
   "source": [
    "Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "6c5773b3-97bd-466d-8322-c4f70f01f79a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ADMIN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\ADMIN\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')  # Optional but helps with WordNet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "46aec0ad-9ff7-4ac1-9068-4ce46b8fc7e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word                Lemma               \n",
      "He                  He                  \n",
      "was                 be                  \n",
      "running             run                 \n",
      "and                 and                 \n",
      "eating              eat                 \n",
      "at                  at                  \n",
      "same                same                \n",
      "time                time                \n",
      "He                  He                  \n",
      "has                 have                \n",
      "bad                 bad                 \n",
      "habit               habit               \n",
      "of                  of                  \n",
      "swimming            swim                \n",
      "after               after               \n",
      "playing             play                \n",
      "long                long                \n",
      "hours               hours               \n",
      "in                  in                  \n",
      "sun                 sun                 \n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer= WordNetLemmatizer()\n",
    "\n",
    "sentence= \"He was running and eating at same time. He has bad habit of swimming after playing long hours in sun.\"\n",
    "punctuations=\"?:!.,;\"\n",
    "sentence_words= nltk.word_tokenize(sentence)\n",
    "for word in sentence_words:\n",
    "    if word in punctuations:\n",
    "        sentence_words.remove(word)\n",
    "\n",
    "sentence_words\n",
    "print(\"{0:20}{1:20}\".format(\"Word\", \"Lemma\"))\n",
    "for word in sentence_words:\n",
    "    print(\"{0:20}{1:20}\".format(word, wordnet_lemmatizer.lemmatize(word, pos='v')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa18c6a2-0328-4f66-a375-c97b3f785222",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
